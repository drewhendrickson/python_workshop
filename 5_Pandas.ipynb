{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Pandas\n",
    "\n",
    "Pandas is a python library designed to make many common types of data analysis straightforward. It borrows heavily from R. \n",
    "\n",
    "### Memory structure\n",
    "\n",
    "The basic datastructure in Pandas is a DataFrame. These data frames are very similar to Numpy Structured Arrays, except how they are stored in memory is completely different. \n",
    "\n",
    "Structured arrays consist of a Numpy array with an entry for each row in the structured array, and each entry is a tuple. \n",
    "\n",
    "Pandas data frames consist of a dictionary-like object (with keys indicating rows or indexes) containing a set of columns. Each column is stored as a Numpy 1D array (actually, Pandas has a wapper for the 1D array called a Series that shows up here and there) that consists of that array plus an additional array that contains indexes.\n",
    "\n",
    "This hybrid representation makes computations within a column very fast, and computations across columns not particularly ineffient because of the extra index array stored with each column. The sacrifice in this case is storage space, Pandas dataframes take up more memory than Numpy or Scipy arrays.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "Pandas is designed to a few things very well. It is incredibly useful for grouping data, aggregating and processing these groups, and reshaping data. Support for time-series analyses in Pandas is also strong with built-in date and time representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing Pandas data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read a data file into pandas\n",
    "data = pandas.read_csv(\"iris.txt\", delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the datafile as a csv\n",
    "data.to_csv(\"tmp.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some simple information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first 5 rows\n",
    "print(data.head())\n",
    "# last 5 rows\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"type\"]\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas has some nice default printing options when not passed through print\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing data frames\n",
    "\n",
    "As with any data structure, it is important to be able to access subsets of the dataframe. Pandas has tools for accessing both rows and columns in a number of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index rows with just a set of indices (no comma)\n",
    "print(data[:5])\n",
    "# data[:5,] # causes an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the names of the columns\n",
    "print(data.columns)\n",
    "print(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# single column (note that only the first 5 rows are selected):\n",
    "print(data['sepal_width'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more than one column:\n",
    "print(data[['sepal_width', 'petal_width']][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# boolean indexing works\n",
    "print(data[(data['type'] == \"virginica\") & (data['petal_length'] < 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rows can also have names:\n",
    "summary = data.describe()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and we can index rows using the loc property:\n",
    "print(summary.loc['mean'])\n",
    "print()\n",
    "print(summary.loc[['mean', '50%']])\n",
    "print()\n",
    "print(summary.loc['25%':'75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or by row index number using iloc:\n",
    "print(summary.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping and pivot tables\n",
    "\n",
    "### Summarizing data\n",
    "\n",
    "Perhaps one of the most useful way to summarize data and understand it is to simply count entries. Here we show a few exaples of recoding continuous values into a binary value and then calcuating table counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some summary functions like crosstab work with selected columns\n",
    "pandas.crosstab(data['petal_width'] > 2, columns=data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rows can be nested hierarchically\n",
    "p_w_2 = data['petal_width'] > 2\n",
    "p_l_2 = data['petal_length'] > 2\n",
    "pandas.crosstab([p_w_2, p_l_2], columns=data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns can be nested hierarchically\n",
    "pandas.crosstab(data['type'], [p_w_2, p_l_2], \n",
    "                colnames=[\"Width > 2\", \"Lenght > 2\"], \n",
    "                rownames=[\"Flower Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count up the values\n",
    "data['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "\n",
    "Pivot tables are a useful tool for summarizing data along different groupings. Pandas provides a wapper function called pivot_table for constructing these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = pandas.DataFrame({'A': ['one', 'one', 'two', 'three'] * 6,\n",
    "                       'B': ['X', 'Y', 'Z'] * 8,\n",
    "                       'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n",
    "                       'D': numpy.random.randn(24),\n",
    "                       'E': numpy.random.randn(24)})\n",
    "example.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a pivot table:\n",
    "# columns parameter specifies the column types, index specifies the rows\n",
    "pandas.pivot_table(example, values='D', index=['A', 'B'], columns=['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more than one column in the dataframe can be used for a value\n",
    "# and pandas will automatically guess which columns to use\n",
    "pandas.pivot_table(example, index=['A', 'B'], columns=['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more than one column can be specified\n",
    "pandas.pivot_table(example, values=['D','E'], index=['B'], columns=['A', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# default aggregation is mean\n",
    "# but we can specify other aggreation functions:\n",
    "pandas.pivot_table(example, values='D', index=['B'], \n",
    "                   columns=['A', 'C'], aggfunc=numpy.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pivot_table has many additional optional parameters but margins is a useful one:\n",
    "# note that we are also using pivot_table as a method of the dataframe\n",
    "example.pivot_table(index=['A', 'B'], columns='C', \n",
    "                    margins=True, aggfunc=numpy.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby\n",
    "\n",
    "One of the most powerful features in pandas is the ability to group dataframes by specific values and then perform calculations on those groups. These features are directly stolen from how R does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar', \n",
    "                              'foo', 'bar', 'foo', 'foo'],\n",
    "                       'B' : ['one', 'one', 'two', 'three', \n",
    "                              'two', 'two', 'one', 'three'],\n",
    "                       'C' : numpy.random.randn(8),\n",
    "                       'D' : numpy.random.randn(8)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group by a single column\n",
    "df.groupby('A').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group by more than one column\n",
    "df.groupby(['A', 'B']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# by default, groupby sorts the groups (alphabetically or numerically)\n",
    "print(df.groupby('B').mean())\n",
    "print()\n",
    "print(df.groupby('B', sort=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing groups and iterating\n",
    "\n",
    "The object created by the function groupy() is actually a list that can be iterated across to perform actions on each group of the data. Pandas provides some vectorized functions and others can be written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can access a single group\n",
    "print(df.groupby('A').get_group('bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through all groups\n",
    "for name, group in df.groupby(['A', 'B']):\n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "Aggregation changes the number of rows or indices within the dataframe. We already saw one way to aggregate a group: describe(). There are many, many others:\n",
    "\n",
    "- count\n",
    "- cumcount\n",
    "- first\n",
    "- head\n",
    "- last\n",
    "- max\n",
    "- mean\n",
    "- median\n",
    "- min\n",
    "- nth\n",
    "- prod\n",
    "- size\n",
    "- sem\n",
    "- std\n",
    "- sum\n",
    "- var\n",
    "- tail\n",
    "- agg\n",
    "- all\n",
    "- any\n",
    "- bfill\n",
    "- corr\n",
    "- count\n",
    "- cov\n",
    "- cummax\n",
    "- cummin\n",
    "- cumprod\n",
    "- cumsum\n",
    "- describe\n",
    "- diff\n",
    "- ffill\n",
    "- fillna\n",
    "- idxmax\n",
    "- idxmin\n",
    "- mad\n",
    "- pct_change\n",
    "- quantile\n",
    "- rank\n",
    "- resample\n",
    "- shift\n",
    "- size\n",
    "- skew\n",
    "- take\n",
    "- tshift\n",
    "- nlargest\n",
    "- nsmallest\n",
    "- value_counts\n",
    "- corrwith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a few examples:\n",
    "print(df.groupby('A').size())\n",
    "print()\n",
    "print(df.groupby('A').max())\n",
    "print()\n",
    "print(df.groupby('A').agg([numpy.sum, numpy.mean, numpy.std]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinate, Join, and Merge\n",
    "\n",
    "### Appending a row or column to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append a row:\n",
    "df = pandas.DataFrame(numpy.random.randn(8, 4), columns=['A','B','C','D'])\n",
    "print(df)\n",
    "s = df.iloc[3]\n",
    "print(df.append(s, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append a column:\n",
    "df['E'] = numpy.random.random(df.shape[0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delete that column\n",
    "del df['E']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatinate data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pandas.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                        'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                        'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                        'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                       index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pandas.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                        'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                        'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                        'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                       index=[4, 5, 6, 7])\n",
    "\n",
    "df3 = pandas.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                        'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                        'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                        'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                       index=[8, 9, 10, 11])\n",
    "\n",
    "df4 = pandas.DataFrame({'E': ['E2', 'E3', 'E6', 'E7'],\n",
    "                        'F': ['F2', 'F3', 'F6', 'F7'],\n",
    "                        'G': ['G2', 'G3', 'G6', 'G7']},\n",
    "                       index=[0, 1, 2, 3])\n",
    "print(df1)\n",
    "print()\n",
    "print(df2)\n",
    "print()\n",
    "print(df3)\n",
    "print()\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it is straightforward to build one dataframe\n",
    "# note: this is numpy.concatinate under the hood so it is expensive in time and memory!\n",
    "dfs = [df1, df2, df3]\n",
    "pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more interestingly, we can add pandas indexes\n",
    "result = pandas.concat(dfs, keys=['x', 'y', 'z'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can then use these indexes to pull out individual dataframes:\n",
    "result.loc['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying rows or columns for the concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas will default to looking to concatinate rows together\n",
    "print(pandas.concat([df1, df4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can specify which axis to merge along:\n",
    "\n",
    "# default behavior\n",
    "print(pandas.concat([df1, df4], axis=0))\n",
    "\n",
    "# concatinate columns with the same index together!\n",
    "print(pandas.concat([df1, df4], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append can also be used but has fewer options:\n",
    "df1.append([df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you are performing many concatinations, \n",
    "# it's much more efficient to use list comprehensions to do a single concatination:\n",
    "if False:\n",
    "    dfs = [ process_your_file(f) for f in files ]\n",
    "    result = pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins using the Concat function\n",
    "\n",
    "Joining data frames is the process of combining two dataframes. This becomes interesting and complex when some (but not all) of the row indicies and column names overlap between the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df5 and df1 share some indicies and names:\n",
    "df5 = pandas.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                        'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                        'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                       index=[2, 3, 6, 7])\n",
    "print(df1)\n",
    "print()\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the default behavior of concat is to not duplicate column names \n",
    "# but to duplicate row indices\n",
    "pandas.concat([df1, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# but the axis parameter can get concat to create duplicate \n",
    "# columns and only unique rows\n",
    "pandas.concat([df1, df5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The default in this case is an outer join:\n",
    "# all rows are retained regardless of each column is present in the row\n",
    "# this can produce NaN values\n",
    "pandas.concat([df1, df5], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can also specify an inner join:\n",
    "# each row must have matching columns\n",
    "# this can lead to dropping rows\n",
    "pandas.concat([df1, df5], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concat can also be told to ignore duplicate indices\n",
    "# this results in new index values being created for conflicts\n",
    "print(df1)\n",
    "print()\n",
    "print(df5)\n",
    "pandas.concat([df1, df5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins using the Join function\n",
    "\n",
    "Join is a convenience function in pandas for joining two dataframes. It is a wrapper around the merge function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left = pandas.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                         'B': ['B0', 'B1', 'B2']},\n",
    "                        index=['K0', 'K1', 'K2'])\n",
    "right = pandas.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "                          'D': ['D0', 'D2', 'D3']},\n",
    "                         index=['K0', 'K2', 'K3'])\n",
    "print(left)\n",
    "print()\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# default is a left join: \n",
    "# all indexes from the left dataframe are kept as well as \n",
    "# any indexes in the right that occur in the left\n",
    "left.join(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also specify a right join\n",
    "left.join(right, how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or an outer join that keeps all indexes\n",
    "left.join(right, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or an inner join that keep all complete indexes\n",
    "left.join(right, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining by key\n",
    "\n",
    "Often datasets are related by a shared key. These are often referred to as relational datasets or databases. Pandas makes it easy to merge across shared key values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "books = pandas.DataFrame({'title': ['Title1', 'Title2', 'Title3', 'Year4'],\n",
    "                         'year': ['Year1', 'Year2', 'Year3', 'Year4'],\n",
    "                         'key': ['A1', 'A2', 'A1', 'A2']})\n",
    "authors = pandas.DataFrame({'first_name': ['F1', 'F2', 'F3'],\n",
    "                          'last_name': ['L1', 'L2', 'L3']},\n",
    "                          index=['A1', 'A2', 'A3'])\n",
    "print(books)\n",
    "print()\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "books.join(authors, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins using the merge function\n",
    "\n",
    "Merge has more options than Join. Specifically, it is possible to specify a key in both dataframes to merge based on, to specify multiple keys within each dataframe, etc.\n",
    "\n",
    "One really nice feature (that is new in 0.17 but not in the stable release on this server) is to include a new column indicating which dataframe the row came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# currently broken but will be cool:\n",
    "# pandas.merge(df1, df5, how='outer', indicator='indicator_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series\n",
    "\n",
    "Pandas has incredibly powerful tools for processing data based on time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a range of dates\n",
    "rng = pandas.date_range('1/1/2011', periods=24*8, freq='H')\n",
    "print(rng.summary())\n",
    "\n",
    "# a series of numbers with those dates as an index\n",
    "ts = pandas.Series(numpy.random.randint(0, 500, len(rng)), index=rng)\n",
    "print(ts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert time zone\n",
    "ts_utc = ts.tz_localize('UTC')\n",
    "print(ts_utc.head())\n",
    "\n",
    "# convert to another time zone\n",
    "print(ts_utc.tz_convert('US/Eastern').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the time increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recode the frequency of the time steps\n",
    "# fill in missing data from the previous step\n",
    "print(ts.asfreq('45Min', method='pad').head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bin the timestamps into days (from hours) and return the mean value \n",
    "print(ts.resample('D', how=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute rolling means\n",
    "lag = pandas.rolling_mean(ts, 5, min_periods=1)\n",
    "print(lag.head(10))\n",
    "print()\n",
    "\n",
    "# compute rolling max\n",
    "roll_max = pandas.rolling_max(ts, 3, min_periods=1)\n",
    "print(roll_max.head(10))\n",
    "\n",
    "# 10+ rolling functions in pandas \n",
    "# including a rolling_apply that takes an arbitrary function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from strings to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converting \n",
    "print(pandas.to_datetime(pandas.Series(['Jul 31, 2009', '2010-01-10', None])))\n",
    "print()\n",
    "print(pandas.to_datetime(['2005/11/23', '2010.12.31']))\n",
    "print()\n",
    "\n",
    "# European style dates:\n",
    "print(pandas.to_datetime(['04-01-2012 10:00'], dayfirst=True))\n",
    "print()\n",
    "print(pandas.to_datetime(['14-01-2012', '01-14-2012'], dayfirst=True))\n",
    "print()\n",
    "\n",
    "# from epochs:\n",
    "print(pandas.to_datetime([1349720105, 1349806505, 1349892905, \n",
    "                          1349979305, 1350065705], unit='s'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
